{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 长文本总结\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.docstore.document import Document as LangDoc\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai.error import RateLimitError\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.output_parsers import RegexParser\n",
    "\n",
    "\n",
    "openai.api_key = \"\"\n",
    "\n",
    "summary_prompt = (\n",
    "\"总结以下会议记录中所探讨的主要话题，忽略细节\\n\"\n",
    "\"会议记录：{text}\\n\"\n",
    "\"在输出时，请注意以下几点：\\n\"\n",
    "\"1. 输出内容中避免口语化内容\\n\"\n",
    "\"2. 每个话题用序号标注\\n\"\n",
    "\"3. 不输出无关信息\"\n",
    ")\n",
    "\n",
    "qa_prompt = \"\"\"\n",
    "结合下面的信息，用中文回答最后的问题。如果你不知道答案，说“我不知道”，不可以编造答案。\n",
    "除了回答问题外，还需要输出一个分数，表示你对这个问题的回答的自信程度。分数越高，你越自信。按照以下的格式输出：\n",
    "\n",
    "回答：[回答内容]\n",
    "分数：[0到100间的数字]\n",
    "\n",
    "开始回答：\n",
    "{context}\n",
    "\n",
    "问题：{question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_chatgpt_reply(query, context=[]):\n",
    "    context += [query]\n",
    "\n",
    "    llm_chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", max_tokens=2000, temperature=0.3)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    docsearch = Chroma(persist_directory=\"./VectorDB\", embedding_function=embeddings)\n",
    "\n",
    "    output_parser = RegexParser(\n",
    "        regex=r\"回答：(.*)\\n*分数：([0-9]*).*\",\n",
    "        output_keys=[\"answer\", \"score\"],\n",
    "    )\n",
    "    PROMPT = PromptTemplate(\n",
    "        template=qa_prompt, input_variables=[\"context\", \"question\"], output_parser=output_parser\n",
    "    )\n",
    "    chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "    qa = RetrievalQA.from_chain_type(llm_chat, chain_type=\"map_rerank\", retriever=docsearch.as_retriever(), chain_type_kwargs=chain_type_kwargs)\n",
    "    result = qa.run(query)\n",
    "\n",
    "    context += [result]\n",
    "    responses = [(u,b) for u,b in zip(context[::2], context[1::2])]\n",
    "    return responses, context\n",
    "\n",
    "\n",
    "def get_chatgpt_summary(content):\n",
    "    texts = text_splitter.split_text(content)\n",
    "    docs = [LangDoc(page_content=t) for t in texts]\n",
    "    llm_summary = OpenAI(model_name=\"gpt-3.5-turbo\", max_tokens=300, temperature=0.2)\n",
    "    each_round_template = PromptTemplate(input_variables=[\"text\"], template=summary_prompt)\n",
    "    chain_summary = load_summarize_chain(llm_summary, chain_type=\"stuff\", prompt=each_round_template)\n",
    "    summary = \"\\n*******\\n\".join([chain_summary.run([doc]) for doc in docs])\n",
    "    return summary\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: ./VectorDB\n",
      "Using embedded DuckDB with persistence: data will be stored in: ./VectorDB\n",
      "Using embedded DuckDB with persistence: data will be stored in: ./VectorDB\n",
      "Using embedded DuckDB with persistence: data will be stored in: ./VectorDB\n",
      "Using embedded DuckDB with persistence: data will be stored in: ./VectorDB\n",
      "Using embedded DuckDB with persistence: data will be stored in: ./VectorDB\n",
      "Using embedded DuckDB with persistence: data will be stored in: ./VectorDB\n",
      "Using embedded DuckDB with persistence: data will be stored in: ./VectorDB\n",
      "C:\\Users\\86139\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain\\llms\\openai.py:169: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "C:\\Users\\86139\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain\\llms\\openai.py:696: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 5b6b9f03ba53a737ab224a7839255396 in your message.).\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from docx import Document\n",
    "import os\n",
    "\n",
    "\n",
    "def upload_file(file):\n",
    "    doc = Document(file.name)\n",
    "    content = \"\"\n",
    "    for para in doc.paragraphs:\n",
    "        content += para.text\n",
    "        content += '\\n'\n",
    "    texts = text_splitter.split_text(content)\n",
    "    docs = [LangDoc(page_content=t) for t in texts]\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    docsearch = Chroma.from_documents(docs, embeddings, persist_directory=\"./VectorDB\")\n",
    "    docsearch.persist()\n",
    "    return content\n",
    "\n",
    "\n",
    "def set_api_key(api_key):\n",
    "    openai.api_key = api_key\n",
    "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "    return None\n",
    "\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Default(text_size='lg', radius_size='sm')) as demo:\n",
    "    with gr.Column():\n",
    "        # 产品介绍\n",
    "        title = gr.Markdown(\"# <center>ChatMeeting</center>\")\n",
    "        desc = gr.Markdown(\"<center>让AI帮你整理会议纪要\\n\\n支持.docx文件</center>\")\n",
    "    \n",
    "    with gr.Column():\n",
    "        # api key\n",
    "        api_input = gr.Textbox(label=\"API Key\", placeholder=\"请输入API Key\", type=\"password\")\n",
    "        api_btn = gr.Button(value=\"设置\")\n",
    "        api_btn.click(fn=set_api_key, inputs=api_input, outputs=None)\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            # 文件上传\n",
    "            file_input = gr.File(file_types=[\".docx\"], label=\"原始文稿\", interactive=True)\n",
    "            upload_btn = gr.Button(value=\"上传\")\n",
    "            # 文字展示\n",
    "            with gr.Tab(\"原文\"):\n",
    "                # 原文\n",
    "                content_box = gr.Textbox(label=\"文稿内容\")\n",
    "            with gr.Tab(\"总结\"):\n",
    "                # 总结\n",
    "                summary_box = gr.Textbox(label=\"总结内容\")\n",
    "        with gr.Column():\n",
    "            # 对话交互\n",
    "            chatbot = gr.Chatbot(label=\"对话内容\").style(height=500)\n",
    "            state = gr.State([])\n",
    "            txt = gr.Textbox(label=\"用户\", placeholder=\"请输入内容\")\n",
    "            with gr.Row():\n",
    "                summary = gr.Button(value=\"一键总结\")\n",
    "                clear = gr.Button(value=\"清空\")\n",
    "            \n",
    "            summary.click(fn=get_chatgpt_summary, inputs=content_box, outputs=summary_box)\n",
    "            txt.submit(get_chatgpt_reply, [txt, state], [chatbot, state])\n",
    "            clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "    # 上传文件，langchain解析\n",
    "    upload_btn.click(fn=upload_file, inputs=file_input, outputs=content_box)\n",
    "\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
